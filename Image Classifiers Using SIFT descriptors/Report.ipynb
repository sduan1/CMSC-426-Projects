{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMSC426, Project03, Shiyuan Duan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project we are implementing an image classfier. We are using SIFT algrithm to generate a Kx128 matrix for each image where K is the number of descriptors we are using. Then we collect all descriptors generated by each image and form a NKx128 matrix where N is the number of image. We will use KMeans clustering algrithm to cluster thoes data in 128 dimensional space. The clusters are the visual bags. For each image we will convert the descriptors into a histogram and use the histogram in an SVM model to classify our images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIFT implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important part of this project is the implementation of SIFT algrithm. Idealy the SIFT algrithem should take an image and return a list of 128 dimension descriptors. SIFT can be break down to several steps and the following report will discuss each step in detail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Scale Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the very first part of SIFT. In this part we create a difference of gaussian pyrimid (In theory we should use laplacian guassians but it is computationally expensive, and DoG is a good approximation of LoG therefore we will be using DoG instead). The idea is that in the gaussian pyrimid thereare 6 octaves and in each octave there are 5 DoG. By constructing the pyrimid we can achieve scale invariant in this algrithm. This is achieved by executing the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # creating feature space\n",
    "    (oct1_h, oct1_w) = np.shape(test_img)\n",
    "    octave1 = [cv2.GaussianBlur(test_img, (5,5), 2**(i/5)).astype(np.float64) for i in range(6)]\n",
    "    dof1 = [abs(octave1[i+1] - octave1[i])/255 for i in range(5)]\n",
    "\n",
    "    test_img2 = cv2.resize(test_img,((int(np.shape(test_img)[1]/2), int(np.shape(test_img)[0]/2))))\n",
    "    octave2 = [cv2.GaussianBlur(test_img2, (5,5), 2**(i/5)).astype(np.float64) for i in range(6)]\n",
    "    dof2 = [abs(octave2[i+1] - octave2[i])/255 for i in range(5)]\n",
    "\n",
    "    test_img3 = cv2.resize(test_img,((int(np.shape(test_img2)[1]/2), int(np.shape(test_img2)[0]/2))))\n",
    "    octave3 = [cv2.GaussianBlur(test_img3, (5,5), 2**(i/5)).astype(np.float64) for i in range(6)]\n",
    "    dof3 = [abs(octave3[i+1] - octave3[i])/255 for i in range(5)]\n",
    "\n",
    "    test_img4 = cv2.resize(test_img,((int(np.shape(test_img3)[1]/2), int(np.shape(test_img3)[0]/2))))\n",
    "    octave4 = [cv2.GaussianBlur(test_img4, (5,5), 2**(i/5)).astype(np.float64) for i in range(6)]\n",
    "    dof4 = [abs(octave4[i+1] - octave4[i])/255 for i in range(5)]\n",
    "\n",
    "    test_img5 = cv2.resize(test_img,((int(np.shape(test_img4)[1]/2), int(np.shape(test_img4)[0]/2))))\n",
    "    octave5 = [cv2.GaussianBlur(test_img5, (5,5), 2**(i/5)).astype(np.float64) for i in range(6)]\n",
    "    dof5 = [abs(octave5[i+1] - octave5[i])/255 for i in range(5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local extrema detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each DoG image(except for the top and bottom), we look for the local min/max compared with it's neighboors around it and on s+1 and s-1 DoG. This step is essentially locating the candidate key points. This step can be simple with the help of helper function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keypoint localization and thresh holding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous step we have located the candidate key points, but we actually find a lot of them and not all of them are useful to us and not all of them are the real extrema. In this step we will locate the real extrema and get rid of the keypoints that are not useful to us. A detailed code is shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def localize_kps(imgs, x, y, s):\n",
    "    dx_kernal = np.array([[0,1/2,0],[0,0,0],[0,-1/2,0]])\n",
    "    dy_kernal = np.array([[0,0,0],[1/2,0,-1/2],[0,0,0]])\n",
    "    dxx_kernal = np.array([[0,0,0],[1,-2,1],[0,0,0]])\n",
    "    dyy_kernal = np.array([[0,1,0],[0,-2,0],[0,1,0]])\n",
    "    dxy_kernal = np.array([[-1,0,1],[0,0,0],[1,0,-1]])\n",
    "                          \n",
    "    dx_img = cv2.filter2D(imgs[s], -1, dx_kernal)\n",
    "    dy_img = cv2.filter2D(imgs[s], -1, dy_kernal)\n",
    "    dxx_img = cv2.filter2D(imgs[s], -1, dxx_kernal)\n",
    "    dxy_img = cv2.filter2D(imgs[s], -1, dxy_kernal)\n",
    "    dyy_img = cv2.filter2D(imgs[s], -1, dyy_kernal)\n",
    "    \n",
    "    dx_img_prev = cv2.filter2D(imgs[s-1], -1, dx_kernal)\n",
    "    dx_img_next = cv2.filter2D(imgs[s+1], -1, dx_kernal)\n",
    "    \n",
    "    dy_img_prev = cv2.filter2D(imgs[s-1], -1, dy_kernal)\n",
    "    dy_img_next = cv2.filter2D(imgs[s+1], -1, dy_kernal)\n",
    "    \n",
    "    \n",
    "    dx = dx_img[y, x]\n",
    "    dy = dy_img[y, x]\n",
    "    ds = imgs[s+1][y, x] - imgs[s-1][y,x]\n",
    "    dxx = dxx_img[y, x]\n",
    "    dxy = dxy_img[y, x]\n",
    "    dyy = dyy_img[y, x]\n",
    "    dxs = (dx_img_prev[y, x] - dx_img_next[y, x])/2\n",
    "    dys = (dy_img_prev[y, x] - dy_img_next[y, x])/2\n",
    "    dss = imgs[s+1][y,x] - 2*imgs[s][y,x] + imgs[s-1][y,x]\n",
    "    \n",
    "    J = np.array([dx, dy, ds]) \n",
    "    HD = np.array([ [dxx, dxy, dxs], [dxy, dyy, dys], [dxs, dys, dss]])\n",
    "    \n",
    "    \n",
    "    offset = -np.linalg.pinv(HD).dot(J)\n",
    "    return offset, J, HD[:2,:2], x, y, s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orientation assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have successfully located all the keypoints but we have not yet achieved orientation assignment. The idea in this step is that there is a dominant orientation in each keypoints. We determine the dominant key point by selecting a patch around the key point(the patch size is determined by the scale). Then we cast it with gaussian kernel of $1.5\\sigma$. Finally we calculate each pixel's orientation and magnitude and put them in a histogram of 36 bins each representing 10 degrees. The bin with highest value is the dominant orientation. In the paper, it is mentioned that any bins exceeding 80% of the highest bin will also be counted as a keypoint with different orientation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptor creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can create the descriptor. To make it rotation invariant, I first take a patch of size roughly $16*sqrt(2)$ and rotate the patch. This is to garentee that after rotating I can see select 16x16 patch and do not loss any information. Then the 16x16 patch is selected and split into 16 sub_regions each with 4x4 dimension. Then it is similar to the orientation assignment step but this time we are using an 8-bin histogram on each subregion. Finally we will get a 4*4*8 = 128 dimensional matrix. We do this for each key point and we will obtain our Kx128 matrix. A detailed implementation is shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_descriptor(new_kp, D_img):\n",
    "    [y, x, theta] = new_kp\n",
    "    x = int(x)\n",
    "    y = int(y)\n",
    "    try:\n",
    "        kp_sub_area = D_img[y-13:y+13,x-13:x+13]\n",
    "        plt.imshow(kp_sub_area,cmap='gray')\n",
    "        M = cv2.getRotationMatrix2D((13,13), theta, 1.0)\n",
    "        rotated_sub_area = cv2.warpAffine(kp_sub_area, M, np.shape(kp_sub_area))\n",
    "\n",
    "        dx_kernal = np.array([[0,1/2,0],[0,0,0],[0,-1/2,0]])\n",
    "        dy_kernal = np.array([[0,0,0],[1/2,0,-1/2],[0,0,0]])\n",
    "        dx_img = cv2.filter2D(rotated_sub_area, -1, dx_kernal)+1e-15\n",
    "        dy_img = cv2.filter2D(rotated_sub_area, -1, dy_kernal)+1e-15\n",
    "\n",
    "        angle_img = np.degrees(np.arctan(dy_img/dx_img))%360\n",
    "\n",
    "        angle_subarea = angle_img[13-8:13+8, 13-8:13+8]\n",
    "        descriptor = np.array(subarea_to_descriptor(angle_subarea))\n",
    "        descriptor = np.ndarray.flatten(descriptor)\n",
    "\n",
    "        return descriptor\n",
    "    except Exception as e:\n",
    "        return np.reshape(np.array([]),(0,128))\n",
    "    \n",
    "    \n",
    "def subarea_to_descriptor(sub_area):\n",
    "    sub_regions = [sub_area[y:y+4, x:x+4] for x in range(0,16,4) for y in range(0,16,4)]\n",
    "    \n",
    "    descriptor = []\n",
    "    for sub_region in sub_regions:\n",
    "        descriptor.append(sub_region_to_hist(sub_region))\n",
    "    return descriptor\n",
    "        \n",
    "        \n",
    "def sub_region_to_hist(sub_region):\n",
    "    kernel = np.array([[1,3,3,1],[3,9,9,3],[3,9,9,3],[1,3,3,1]])/64\n",
    "    hist = np.zeros(8, dtype = np.float32)\n",
    "    for y in range(4):\n",
    "        for x in range(4):\n",
    "            hist[int(sub_region[y,x]//45)] += sub_region[y,x]*kernel[y,x]\n",
    "            \n",
    "    hist /= np.linalg.norm(hist)\n",
    "    hist[hist>0.2] = 0.2\n",
    "    hist /= np.linalg.norm(hist)\n",
    "    \n",
    "    return hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can put everything in one function img_to_descriptors(img). This can be found in code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating bags of visual words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we are creating bags of visual words. This is done by stacking all the descriptors in training image and use kmeans to cluster them. Each cluster should represent a bag of visual word. This step is trivial with the help of sklearn.cluster.KMean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting image into a histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have created the bags of visual words, we can convert our image to a histogram. The idea is that the descriptors are clustered and classified into different groups of visual word. For any given image we can calculate the descriptors and classify all the descriptors based on our kmeans algrithm. We construct the histogram by the frequency in each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training is the final step our this project and it is the most trivial part. We generate a histogram for each training image and label them for a SVM model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are shown in the last blocks in the code file. It shows the confusion matrix, a sample histogram of an image and a sample bag of visual word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the confusion matrix, we achieved accuarcy of 84% for airplanes and Leopard. However, our model performed poorly on dolphin with accuarcy of 67%. This may because we only used 50 images for training set and 15 images for testing. There's may not be enough training images. Also dolphin maybe hard to classify because dolphin do not have distinctive pattern liek leopard and airplanes. The images are too smooth resulting less number of keypoints."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
